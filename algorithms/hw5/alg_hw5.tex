% HW Template for CS 6150, taken from https://www.cs.cmu.edu/~ckingsf/class/02-714/hw-template.tex
%
% You don't need to use LaTeX or this template, but you must turn your homework in as
% a typeset PDF somehow.
%
% How to use:
%    1. Update your information in section "A" below
%    2. Write your answers in section "B" below. Precede answers for all 
%       parts of a question with the command "\question{n}{desc}" where n is
%       the question number and "desc" is a short, one-line description of 
%       the problem. There is no need to restate the problem.
%    3. If a question has multiple parts, precede the answer to part x with the
%       command "\part{x}".
%    4. If a problem asks you to design an algorithm, use the commands
%       \algorithm, \correctness, \runtime to precede your discussion of the 
%       description of the algorithm, its correctness, and its running time, respectively.
%    5. You can include graphics by using the command \includegraphics{FILENAME}
%
\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage{fancyhdr}
\usepackage{framed}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{pifont}
\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt plus 1pt}
\setlength{\headheight}{13.6pt}
\newcommand\question[2]{\vspace{.25in}\hrule\textbf{#1: #2}\vspace{.5em}\hrule\vspace{.10in}}
\renewcommand\part[1]{\vspace{.10in}\textbf{(#1)}}
\newcommand\algorith{\vspace{.10in}\textbf{Algorithm: }}
\newcommand\correctness{\vspace{.10in}\textbf{Correctness: }}
\newcommand\runtime{\vspace{.10in}\textbf{Running time: }}
\pagestyle{fancyplain}
\lhead{\textbf{\NAME\ (\UID)}}
\chead{\textbf{HW\HWNUM}}
\rhead{CS 6150, \today}
\begin{document}\raggedright
%Section A==============Change the values below to match your information==================
\newcommand\NAME{Jake Pitkin}  % your name
\newcommand\UID{u0891770}     % your utah UID
\newcommand\HWNUM{5}              % the homework number
%Section B==============Put your answers to the questions below here=======================

\question{1}{Balls and Bins}
\begin{equation}
\setlength\fboxsep{0.25cm}
\setlength\fboxrule{0.4pt}
\boxed{Markov's \ Inequality: \ Pr(X \geq a) \leq \frac{E[x]}{a}}
\end{equation}

\part{a} Given $n$ bins and $4 n \log n$ balls, we want to prove that the probability that there exists an empty bin is $< 1/n$.

\textbf{Proof} We will prove this using Markov's Inequality. Let $X$ be a random variable representing the number of empty bins. We will show that the probability that at least one bin is empty is $< 1/n$.

$$Pr(X \geq 1) \leq E[X]$$

From class, we saw the expectation of X is equivalent to the summation of each of the bins. Let $y_i$ represent each bin where a 1 represents an empty bin and a 0 otherwise.

$$Pr(X \geq 1) \leq E[X] = E[\sum_{i = 1}^n y_i]$$

The probability a bin is empty is given by $(1 - 1/n)^m$ where $m$ is the number of balls. That is, there are $1 - 1/n$ other bins each ball can be placed in. 

$$Pr(X \geq 1) \leq n * (1 - \frac{1}{n})^{4 n log n}$$

From Bernoulli's inequality we know that $1 + y \leq e^y$ for all for all $y$. Letting $y = \frac{-1}{n}$ will allow us to substitute $1 + y$ for $e^y$. This is valid as $1 + y \leq e^y$ so Markov's Inequality still holds.

$$Pr(X \geq 1) \leq n * (1 + y)^{4 n log n}$$
$$Pr(X \geq 1) \leq n * (e^{y})^{4 n log n}$$
$$Pr(X \geq 1) \leq n * (e^{-1/n})^{4 n log n}$$

Applying exponent and natural log identities.

$$Pr(X \geq 1) \leq n * e^{-4 log n}$$
$$Pr(X \geq 1) \leq \frac{1}{n^3} < \frac{1}{n}$$

Thus proving the probability that at least one bin is empty is $< 1/n$.

\part{b.a} When $m = \frac{1}{2} n log n$, the logic follows as in part a.

$$Pr(X \geq 1) \leq E[X] = E[\sum_{i = 1}^n y_i] = n * (1 - \frac{1}{n})^{\frac{1}{2} n log n}$$

Where $y_i$ is a bin and 1 represents an empty bin and 0 otherwise. Making the similar substitution and applying similar identities as part a.

$$Pr(X \geq 1) \leq n * (e^{-1/n})^{\frac{1}{2} n log n}$$
$$Pr(X \geq 1) \leq n * \frac{1}{\sqrt{n}}$$
$$Pr(X \geq 1) \leq \sqrt{n}$$

Given $\frac{1}{2} n log n$ balls, we can say that the probability that at least one bin is empty is $\leq \sqrt{n}$.

\framebox[1.2\width]{$Pr(X \geq 1) \leq \sqrt{n}$}

\part{b.b} When $m = 100 n log n$, a similar logic follows.

$$Pr(X \geq 1) \leq E[X] = E[\sum_{i = 1}^n y_i] = n * (1 - \frac{1}{n})^{100 n log n}$$
$$Pr(X \geq 1) \leq n * (e^{-1/n})^{100 n log n}$$
$$Pr(X \geq 1) \leq n * \frac{1}{n^{100}}$$
$$Pr(X \geq 1) \leq \frac{1}{n^{99}}$$

Given $100 n log n$ balls, we can say that the probability that at least one bin is empty is $\leq \frac{1}{n^{99}}$.

\framebox[1.2\width]{$Pr(X \geq 1) \leq \frac{1}{n^{99}}$}

\part{c} Given n bins and n balls we want to bound the probability that $90\%$ of the bins are empty. Using Markov's Inequality we will derive such a bound. 

$$Pr(X \geq a) \leq \frac{E[X]}{a}$$
$$Pr(X \geq 0.9n) \leq \frac{E[X]}{0.9n}$$

Where X is a random variable representing the number of empty bins. From class we the expectation of X is equivalent to the summation of each of the bins. Let $y_i$ represent each bin where a 1 represents an empty bin and a 0 otherwise.

$$Pr(X \geq 0.9n) \leq \frac{E[\sum_{i = 1}^n y_i]}{0.9n}$$
$$Pr(X \geq 0.9n) \leq \frac{n * (1 - \frac{1}{n})^n}{0.9n}$$
$$Pr(X \geq 0.9n) \leq \frac{n * (e^\frac{-1}{n})^n}{0.9n}$$
$$Pr(X \geq 0.9n) \leq \frac{\frac{1}{e}}{0.9} = \frac{1}{0.9e}$$

Given n balls and n bins, we can say that the probability that $90\%$ of the bins are empty is $\leq \frac{1}{0.9e}$.

\framebox[1.2\width]{$Pr(X \geq 0.9n) \leq \frac{1}{0.9e}$}

\part{d}

\question{2}{Estimating the Mean and Median}

As by the suggestion of the TA Michael Matteny I will be using Hoeffding's Inequality to provide bounds for the following questions rather than the suggested Chernoff bounds. They are related and provide similar strength in bounds as they both provide bounds for \textbf{independent} random variables. This Hoeffding's Inequality stronger than Markov's or Chebyshev's inequalities.

\begin{equation}
\setlength\fboxsep{0.25cm}
\setlength\fboxrule{0.4pt}
\boxed{Hoeffding's \ Inequality: \ P(|X - E[X]| \geq t) \leq 2 exp(\frac{-2n^2t^2}{\sum_{i = 1}^n (b_i - a_i)^2})}
\end{equation}

\part{a} This variation of Hoeffding's Inequality is used when we know that $X_i$'s are strictly in the intervals $[a_i, b_i]$. For our purposes $a_i$ is -1 and $b_i$ is 1. Let X be the sample mean after $j$ random indices are sampled or $\hat\mu$. The expected value of X is the true mean or $\mu$. 

Hoeffding's Inequality tells us the probability that the difference between $X$ and $E[X]$ will be $\geq t$ is less than the RHS of the inequality. That is, if $t$ is $\epsilon$ we want the RHS to be $\leq \delta$. This will give us the number of required samples to ensure that $|\hat \mu - \mu| \leq \epsilon$ with probability $1 - \delta$, where $n$ is the number of samples.

$$P(|\hat \mu - \mu| \geq \epsilon) \leq 2 exp(\frac{-2n^2\epsilon^2}{\sum_{i = 1}^n (1 + 1)^2}) \leq \delta$$
$$exp(\frac{-2n^2\epsilon^2}{4n}) \leq \frac{\delta}{2}$$
$$\frac{-n\epsilon^2}{2} \leq \ln(\frac{\delta}{2})$$
$$n\epsilon^2 \geq ln(\frac{4}{\delta^2})$$
$$n \geq ln(\frac{4}{\delta^2}) * \frac{1}{\epsilon^2}$$

Given a $\delta$ and $\epsilon$, the derived equation produces the $n$ number of indices we must sample to satisfy $|\hat \mu - \mu| \leq \epsilon$ with probability $1 - \delta$.

\part{b} No. If we were to sample without replacement, the sampled $a_j$'s are no longer independent events. They become \textbf{dependent} on what has already been sampled as the sample space is changing with each sample. Hoeffding's Inequality is only applicable for \textbf{independent} random variables.

\part{c} Given the value of each $a_i$ has the constraint $a_i \in [-M, M]$, we can still use Hoeffding's Inequality to derive the require number of samples. The setup will be similar to part a, with the exception that now $a = -M$ and $b = M$. Recall that the generalization of Hoeffding's Inequality is used when we know that $X_i$'s are strictly bounded by the intervals $[a_i, b_i]$.

$$P(|\hat \mu - \mu| \geq \epsilon) \leq 2 exp(\frac{-2n^2\epsilon^2}{\sum_{i = 1}^n (M + M)^2}) \leq \delta$$

$$exp(\frac{-n\epsilon^2}{2M^2}) \leq \frac{\delta}{2}$$
$$\frac{-n\epsilon^2}{2M^2} \leq ln(\frac{\delta}{2})$$
$$\frac{n\epsilon^2}{M^2} \geq ln(\frac{4}{\delta^2})$$
$$n \geq ln(\frac{4}{\delta^2}) * \frac{M^2}{\epsilon^2}$$

Given a $\delta$, $\epsilon$, and a $M$, the derived equation produces the $n$ number of indices we must sample to satisfy $|\hat \mu - \mu| \leq \epsilon$ with probability $1 - \delta$.

\part{d} From the problem, we know that for any distinct indices $j_1, j_2,...,j_k$, we have

$$\mathbf{Inequality \ One: \ }Pr[X_{j1} = 1 | X_{j2} = X_{j3} = ... = X_{jk} = 1] \leq Pr[X_{j1} = 1]$$

From Baye's Rule we have:

$$Pr[A = 1 \ \& \ B = 1] = Pr[A | B] * P[B]$$

We want to prove that the probability that $90\%$ of the bins are empty is at most $(0.9)^n$.

As a first step, we will first use the given inequality and Baye's rule to prove:

$$\mathbf{Inequality \ Two: \ } Pr[X_{j1} = X_{j2} = X_{j3} = ... = X_{jk} = 1] \leq e^{-k}$$

That is, the probability that a given $k$ bins are all empty is $\leq e^{-k}$.

First, we will use Baye's Rule. Assigning two new random variables $A$ and $B$. Where A is $X_{j1} = 1$ and $B$ is $X_{j2} = X_{j3} = ... = X_{jk}$. The goal is to transform the LHS of \textit{Inequality Two} to be identical to the LHS of \textit{Inequality One}.

$$Baye's \ Rule: \ Pr[A = 1 \ \& \ B = 1] = Pr[A|B] * P[B]$$
$$What \ we \ start \ with: \ Pr[X_{j1} = X_{j2} = X_{j3} = ... = X_{jk} = 1]$$
$$Applying \ Baye's \ Rule: \ Pr[X_{j1} = 1 | X_{j2} = .. = X_{jk}] * Pr[X_{j2} = ... = X_{jk}]$$

Now Baye's Rule is applied again to $Pr[X_{j2} = ... = X_{jk}]$ where A is $X_{j2} = 1$ and B is $X_{j3} = ... = X_{jk} = 1$:

$$What \ we \ start \ with: \ Pr[X_{j2} = X_{j3} = X_{j4} = ... = X_{jk} = 1]$$
$$Applying \ Baye's \ Rule: \ Pr[X_{j2} = 1 | X_{j3} = .. = X_{jk}] * Pr[X_{j2} = ... = X_{jk}]$$

This pattern of applying Baye's rule to the $P[B]$ portion is repeated $k$ times. This produces a product of the form:

$$\prod_{i = 1}^k Pr[X_{ji} = 1 | X_{j(i + 1)} = ... = X_{ji} = 1]$$

This is identical in form of the LHS of \textit{Inequality One} if we take a product over both sides of \textit{Inequality One}:

$$\prod_{i = 1}^k Pr[X_{ji} = 1 | X_{j(i + 1)} = ... = X_{ji} = 1] \leq \prod_{i = 1}^k Pr[X_{ji}]$$

Thus, we need to analyze the RHS of the above inequality to prove the bounds for \textit{Inequality Two}. We saw in class that the probability that bin $j$ is empty is $(1 - \frac{1}{n})^n \leq \frac{1}{e}$ where $n$ is the number of balls. Using this, we can take the product of $\frac{1}{e}$ $k$ times.

$$\prod_{i = 1}^k Pr[X_{ji}] \leq \prod_{i = 1}^k \frac{1}{e} = e^{-k}$$

Thus proving \textit{Inequality Two}.

Using \textit{Inequality Two}, we can get a bound on the probability that $90\%$ of the bins are empty. Let $k = 0.9n$.

$$Pr[X_{j1} = X_{j2} = ... = X_{jk} = 1] \leq e^{-0.9n}$$

Returning to the original proof, we want to prove the probability that $90\%$ of the bins are empty is bounded by $0.9^n$. We have proven the bound $e^{-0.9n}$. We know that $n$ is a positive integer as it's the number of bins. For all positive integers $e^{-0.9n } \leq 0.9^n$ holds. Thus we have proven a tighter bound and in the process proven the original bound.

\question{3}{Quick-sort with Optimal Comparisons}

\part{a}

\part{b}

\part{c}

\question{4}{Randomized Min-Cut}

\part{a}

\part{b}

\part{c}

\part{d}

\question{5}{Valiant-Vazirani Lemma}
\end{document}
