% HW Template for CS 6150, taken from https://www.cs.cmu.edu/~ckingsf/class/02-714/hw-template.tex
%
% You don't need to use LaTeX or this template, but you must turn your homework in as
% a typeset PDF somehow.
%
% How to use:
%    1. Update your information in section "A" below
%    2. Write your answers in section "B" below. Precede answers for all 
%       parts of a question with the command "\question{n}{desc}" where n is
%       the question number and "desc" is a short, one-line description of 
%       the problem. There is no need to restate the problem.
%    3. If a question has multiple parts, precede the answer to part x with the
%       command "\part{x}".
%    4. If a problem asks you to design an algorithm, use the commands
%       \algorithm, \correctness, \runtime to precede your discussion of the 
%       description of the algorithm, its correctness, and its running time, respectively.
%    5. You can include graphics by using the command \includegraphics{FILENAME}
%
\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage{fancyhdr}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{pifont}
\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt plus 1pt}
\setlength{\headheight}{13.6pt}
\newcommand\question[2]{\vspace{.25in}\hrule\textbf{#1: #2}\vspace{.5em}\hrule\vspace{.10in}}
\renewcommand\part[1]{\vspace{.10in}\textbf{(#1)}}
\newcommand\algorith{\vspace{.10in}\textbf{Algorithm: }}
\newcommand\correctness{\vspace{.10in}\textbf{Correctness: }}
\newcommand\runtime{\vspace{.10in}\textbf{Running time: }}
\pagestyle{fancyplain}
\lhead{\textbf{\NAME\ (\UID)}}
\chead{\textbf{HW\HWNUM}}
\rhead{CS 6350, \today}
\begin{document}\raggedright
%Section A==============Change the values below to match your information==================
\newcommand\NAME{Jake Pitkin}  % your name
\newcommand\UID{u0891770}     % your utah UID
\newcommand\HWNUM{4}              % the homework number
%Section B==============Put your answers to the questions below here=======================

\question{1}{PAC Learning}

\textbf{Rule 1:} You are free to combine any of the parts as they are.

\textbf{Rule 2:} You may also cut any of the parts into two distinct pieces before using them.

\part{1a} 

Given $N$ parts, each product that can be made out of these parts is a distinct hypothesis $h$ in the hypothesis space $H$. From \textit{Rule 1}, a worker can choose to include or not include any of the parts in a product. This can be viewed as a monotone conjunction as a product is defined by choosing to include or not include each of the $N$ parts. There exists $2^N$ possible products as there are two choices for each of the $N$ parts. We will not consider the product constructed by using none of the parts.
	
	\framebox[1.2\width]{$|H| = 2^N - 1$}

\part{1b}

	The experienced worker now creates a product using \textit{Rule 1} and \textit{Rule 2}. There are now four choices that can be made for each of the parts: don't include it, include it, cut the part and use the first half or cut the part and use the second half. A product is now defined as making four choices for each of the $N$ parts. Thus there are $4^N$ possible products. We will not consider the product constructed by using none of the parts.

	\framebox[1.2\width]{$|H| = 4^N - 1$}

\part{1c}

By applying the principles of Occams's Razor we can make a statement about the number of required examples the robot will have to see to have an error of 0.01 with probability $99\%$ on products with 6 available parts.

Given a hypothesis space $H$, we can say with probability $1 - \delta$, a hypothesis $h \in H$, that is consistent with a training set of size $m$, will have an error $< \epsilon$ on future examples if

$$m > \frac{1}{\epsilon}(ln(|H|) + ln\frac{1}{\delta})$$

We want an error rate of $\epsilon = 0.01$ with probability $1 - \delta = 0.99$ with a $|H| = 4^6 = 4,096$.

$$m > \frac{1}{0.01}(ln(4,095) + ln\frac{1}{0.01})$$
$$m > 1,292.27$$

The robot will have to see at least $1,293$ examples to guarantee a $0.01$ error with probability $99\%$ if there are 6 available parts. We round up as the number of required examples must be an integer value and rounding down would not satisfy the equality.

\framebox[1.2\width]{at least 1,293 examples}
\newpage
\part{2}

\question{2}{VC Dimensions}

\part{1} We want to prove that a finite hypothesis space $\mathcal{C}$ has a VC dimension at most $log_2|\mathcal{C}|$. That is, $VC(\mathcal{C}) \leq log_2|\mathcal{C}|$.

\textbf{Proof by contradiction:} Assume the opposite that $VC(\mathcal{C}) > log_2|C|$ is true.

\textbf{Shatter:} A set of examples S is \textit{shattered} by a set of functions H if for every partition of the examples in S into positive and negative examples there is a function in H that gives exactly these labels to the examples.

\textbf{VC Dimension:} The \textit{VC Dimension} of hypothesis space H over instance space X is the size of the largest finite subset of X that is shattered by H.

Take a set of examples S that is the largest finite subset of X that is shattered by a hypothesis space $\mathcal{C}$ and is of size $d$. There exists $2^d$ (the number of possible binary vectors of length $d$) ways to partition S. Thus $|\mathcal{C}| = 2^d$ as we have $2^d$ hypothesis functions.

$$VC(\mathcal{C}) > log_2|C|$$
The VC Dimension of $\mathcal{C} = |S| = d$ as S is the largest finite subset of X that is shattered by H. The size of the hypothesis class is $2^d$.
$$d > log_2(2^d)$$
$$d > d$$

Arriving at the contradiction $d > d$. Thus proving $VC(\mathcal{C}) \leq log_2|\mathcal{C}|$.

\part{2a}

\part{2b}

\part{3}

\part{4}

\part{5}

\question{3}{AdaBoost}
We can calculate $D_2$ given $h_a$, $\alpha_1$, and $D_1$ for each example in the training set.

\begin{equation}
\setlength\fboxsep{0.25cm}
\setlength\fboxrule{0.4pt}
\boxed{D_{t+1}(i) = \frac{D_t(i)}{Z_t} * exp(-\alpha_t * y_ih_i(x_i))}
\end{equation}

$$D_{2}(i) = \frac{D_1(i)}{Z_1} * exp(-\alpha_1 * y_ih_a(x_i))$$
$$D_2(1) = \frac{1}{2}, \ D_2(2) = \frac{1}{6}, \ D_2(3) = \frac{1}{6}, \ D_2(4) = \frac{1}{6} $$


 \begin{table}[H]
\centering
{\renewcommand{\arraystretch}{1.2}%
\begin{tabular}{| c | c | c | c | c | c |}
\hline
$x = [x_1, x_2]$& $y_i$ & $h_a(x)$ & $D_1$ & $D_1(i)y_ih_t(x_i)$ & $D_2$\\
\hline
[1,1] & -1 & 1 & 1/4 & -1/4 & 1/2\\ \hline
[1,-1] & 1 & 1 & 1/4 & 1/4 & 1/6\\ \hline
[-1,-1] & -1 & -1 & 1/4 & 1/4 & 1/6\\ \hline
[-1,1] & -1 & -1 & 1/4 & 1/4 & 1/6\\ \hline
\end{tabular}}
\caption{$h_a(x) = sgn(x_1), \ \epsilon_1 = 1/4, \ \alpha_1 = \frac{\ln3}{2}, \ Z_1 = \frac{\sqrt{3}}{2}$}
\end{table}

\textbf{Choosing} $\mathbf{h_d(x) = -sgn(x_2)}$ \textbf{for iteration 2.} We calculate the weighted classification error to determine if it's better than chance.

\begin{equation}
\setlength\fboxsep{0.25cm}
\setlength\fboxrule{0.4pt}
\boxed{\epsilon_t = \frac{1}{2} - \frac{1}{2}(\sum_{i=1}^m D_t(i)*y_ih_i(i))}
\end{equation}

$$\epsilon_2 = \frac{1}{2} - \frac{1}{2}(\sum_{i=1}^m D_2(i)*y_ih_d(i)) = \frac{1}{6}$$

Given $\epsilon_2$, we calculate $\alpha_2$ which is the weight the current hypothesis has on the final hypothesis.

\begin{equation}
\setlength\fboxsep{0.25cm}
\setlength\fboxrule{0.4pt}
\boxed{\alpha_t = \frac{1}{2}\ln(\frac{1 - \epsilon_t}{\epsilon_t})}
\end{equation}

$$\alpha_2 = \frac{1}{2}\ln(\frac{1 - \epsilon_2}{\epsilon_2}) = \frac{1}{2}\ln(\frac{1 - \frac{1}{6}}{\frac{1}{6}}) = \frac{\ln5}{2}$$

We calculate $Z_2$ which is a normalization constant to ensure all of the $D_3$ weights add up to 1.
\begin{equation}
\setlength\fboxsep{0.25cm}
\setlength\fboxrule{0.4pt}
\boxed{Z_t = \sum_{i = 1}^m D_t(i) * exp(-\alpha_t * y_ih_i(x_i))}
\end{equation}

$$Z_2 = \sum_{i = 1}^m D_2(i) * exp(-\alpha_2 * y_ih_d(x_i)) = \frac{\sqrt{5}}{3}$$

Finally we calculate a new weight $D_3$ for each example in the training set.

$$D_{3}(i) = \frac{D_2(i)}{Z_2} * exp(-\alpha_2 * y_ih_d(x_i))$$
$$D_3(1) = \frac{3}{10}, \ D_3(2) = \frac{1}{10}, \ D_3(3) = \frac{1}{2}, \ D_3(4) = \frac{1}{10} $$

The results for iteration 2 using hypothesis $h_d(x)$ are recorded in the table below.

\begin{table}[H]
\centering
{\renewcommand{\arraystretch}{1.2}%
\begin{tabular}{| c | c | c | c | c | c |}
\hline
$x = [x_1, x_2]$& $y_i$ & $h_d(x)$ & $D_2$ & $D_1(i)y_ih_t(x_i)$ & $D_3$\\
\hline
[1,1] & -1 & -1 & 1/2 &  1/2& 3/10\\ \hline
[1,-1] & 1 &  1&  1/6&  1/6& 1/10\\ \hline
[-1,-1] & -1 & 1 & 1/6&  -1/6& 1/2\\ \hline
[-1,1] & -1 & -1 &  1/6&  1/6& 1/10\\ \hline
\end{tabular}}
\caption{$h_d(x) = -sgn(x_2), \ \epsilon_2 = 1/6, \ \alpha_2 = \frac{\ln5}{2}, \ Z_2 = \frac{\sqrt{5}}{3}$}
\end{table}

\textbf{Choosing} $\mathbf{h_b(x) = sgn(x_1 - 2)}$ \textbf{for iteration 3.} We calculate the weighted classification error to determine if it's better than chance.

$$\epsilon_3 = \frac{1}{2} - \frac{1}{2}(\sum_{i=1}^m D_3(i)*y_ih_b(i)) = \frac{1}{10}$$

Given $\epsilon_3$, we calculate $\alpha_3$ which is the weight the current hypothesis has on the final hypothesis.

$$\alpha_3 = \frac{1}{2}\ln(\frac{1 - \epsilon_3}{\epsilon_3}) = \frac{1}{2}\ln(\frac{1 - \frac{1}{10}}{\frac{1}{10}}) = \frac{\ln9}{2}$$

We calculate $Z_3$ which is a normalization constant to ensure all of the $D_4$ weights add up to 1.

$$Z_3 = \sum_{i = 1}^m D_3(i) * exp(-\alpha_3 * y_ih_b(x_i)) = \frac{3}{5}$$

Finally we calculate a new weight $D_4$ for each example in the training set.

$$D_{4}(i) = \frac{D_3(i)}{Z_3} * exp(-\alpha_3 * y_ih_b(x_i))$$
$$D_4(1) = \frac{1}{6}, \ D_4(2) = \frac{1}{2}, \ D_4(3) = \frac{5}{18}, \ D_4(4) = \frac{1}{18} $$

The results for iteration 3 using hypothesis $h_b(x)$ are recorded in the table below.

\begin{table}[H]
\centering
{\renewcommand{\arraystretch}{1.2}%
\begin{tabular}{| c | c | c | c | c | c |}
\hline
$x = [x_1, x_2]$& $y_i$ & $h_b(x)$ & $D_3$ & $D_1(i)y_ih_t(x_i)$ & $D_4$\\
\hline
[1,1] & -1 & -1&  3/10&  3/10& 1/6\\ \hline
[1,-1] & 1 &  -1&  1/10&  -1/10& 1/2\\ \hline
[-1,-1] & -1 & -1 & 5/10&  1/2& 5/18\\ \hline
[-1,1] & -1 &  -1&  1/10&  1/10& 1/18\\ \hline
\end{tabular}}
\caption{$h_b(x) = sgn(x_1 - 2), \ \epsilon_1 = 1/10, \ \alpha_1 = \frac{\ln9}{2}, \ Z_3 = \frac{3}{5}$}
\end{table}

\textbf{Choosing} $\mathbf{h_c(x) = -sgn(x_1)}$ \textbf{for iteration 4.} We calculate the weighted classification error to determine if it's better than chance.

The weighted classification error $\epsilon_4$ for $h_c(x)$ is not better than chance.

$$\epsilon_4 = \frac{1}{2} - \frac{1}{2}(\sum_{i=1}^m D_4(i)*y_ih_c(i)) = \frac{5}{6}$$

The classification error $\epsilon_4$ is not better than chance. As a result hypothesis $h_c(x)$ is not considered.

Finally, we consider the final hypothesis $H_{final}(x)$ which takes a weighted average of the classification of $h_a(x)$, $h_b(x)$, and $h_d(x)$.

\begin{equation}
\setlength\fboxsep{0.25cm}
\setlength\fboxrule{0.4pt}
\boxed{H_{final}(x) = sgn(\sum_{t}\alpha_th_t(x))}
\end{equation}

Using $H_{final}(x)$ we classify each example from the training set.

$$H_{final}(1) = sgn(\frac{\ln3}{2}(1) + \frac{\ln5}{2}(-1) + \frac{\ln9}{2}(-1)) = -1$$
$$H_{final}(2) = sgn(\frac{\ln3}{2}(1) + \frac{\ln5}{2}(1) + \frac{\ln9}{2}(-1)) = 1$$
$$H_{final}(3) = sgn(\frac{\ln3}{2}(-1) + \frac{\ln5}{2}(1) + \frac{\ln9}{2}(-1)) = -1$$
$$H_{final}(4) = sgn(\frac{\ln3}{2}(-1) + \frac{\ln5}{2}(-1) + \frac{\ln9}{2}(-1)) = -1$$

\begin{table}[H]
\centering
{\renewcommand{\arraystretch}{1.2}%
\begin{tabular}{| c | c | c |}
\hline
$x = [x_1, x_2]$& $y_i$ & $H_{final}(x)$\\
\hline
[1,1] & -1 & -1\\ \hline
[1,-1] & 1 & 1\\ \hline
[-1,-1] & -1 & -1\\ \hline
[-1,1] & -1 & -1\\ \hline
\end{tabular}}
\caption{Classification of the training set by $H_{final}(x)$}
\end{table}

Using $H_{final}(x)$ we have properly classified all the examples in the training set.

\end{document}
