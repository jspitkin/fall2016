%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Thin Sectioned Essay
% LaTeX Template
% Version 1.0 (3/8/13)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original Author:
% Nicolas Diaz (nsdiaz@uc.cl) with extensive modifications by:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[a4paper, 11pt]{article} % Font size (can be 10pt, 11pt or 12pt) and paper size (remove a4paper for US letter paper)

\usepackage[protrusion=true,expansion=true]{microtype} % Better typography
\usepackage{graphicx} % Required for including pictures
\usepackage{wrapfig} % Allows in-line images
\usepackage{float}
\usepackage{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} % Required for accented characters
\linespread{1.05} % Change line spacing here, Palatino benefits from a slight increase by default

\makeatletter
\renewcommand\@biblabel[1]{\textbf{#1.}} % Change the square brackets for each bibliography item from '[1]' to '1.'
\renewcommand{\@listI}{\itemsep=0pt} % Reduce the space between items in the itemize and enumerate environments and the bibliography

\renewcommand{\maketitle}{ % Customize the title - do not edit title and author name here, see the TITLE block below
\begin{flushright} % Right align
{\LARGE\@title} % Increase the font size of the title

\vspace{50pt} % Some vertical space between the title and author name

{\large\@author} % Author name
\\\@date % Date

\vspace{40pt} % Some vertical space between the author block and abstract
\end{flushright}
}

%----------------------------------------------------------------------------------------
%	TITLE
%----------------------------------------------------------------------------------------

\title{\textbf{Detecting Malware in Android Applications}\\ % Title
A Walk Through a Random Forest} % Subtitle

\author{\textsc{Jake Pitkin} % Author
\\{\textit{CS 6350 - Machine Learning}}} % Institution

\date{\today} % Date

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Print the title section

%----------------------------------------------------------------------------------------
%	ABSTRACT AND KEYWORDS
%----------------------------------------------------------------------------------------

%\renewcommand{\abstractname}{Summary} % Uncomment to change the name of the abstract to something else


%----------------------------------------------------------------------------------------
%	ESSAY BODY
%----------------------------------------------------------------------------------------
\section*{Introduction}

The Android platform is the most popular mobile platform, holding nearly $85\%$ of the global smartphone marketshare. [1] Most of the available software for Android devices comes from public application markets such as Google Play and the Apple App Store. As a result, Android users are a very popular target for malicious software. 

The goal is to classify software as malicious or not as a way to improve the security of the Android platform. This will be accomplished by training a classifier on examples where we observed how often particular system calls were called. The hope is this will expose a patterns of system calls that malicious applications make. Allowing us to prohibit these applications from public markets and protect users.
 
%------------------------------------------------

\section*{Approach - Random Forest}
The approach I used to classify the maliciousness of an Android application is a random forest. A random forest is an ensemble method based on bagging (short for bootstrap aggregating) and a decision tree using the ID3 algorithm. In the classic ID3 algorithm, all features are considered (using the heuristic of information gain to select a feature at each level) and one tree is trained on all the training examples. For bagging, I randomly selected \textit{m} examples with replacement and only consider \textit{k} random features when building the tree. I then built \textit{N} of these trees. 

When it came time to predict a label for a new example, all of the trees in the random forest vote on a label. As I was performing binary classification, I chose $N$ to be odd and took the majority vote of the trees.

\begin{figure} [H]
  \centerline{\includegraphics[width=0.8\linewidth]{decision_tree.png}}
  \caption{Taking the majority vote of an ensemble of decision trees.}
\end{figure}

\section*{Feature Exploration}
Each example is defined by 360 features or system calls. But I discovered only 130 of these features appear in the training examples. Of these 130 features, there are a great deal of them that appear in nearly every training example and four features that appear in every training example. The 65 features with the highest frequency appear in at least half of the training examples.

A feature's value is defined by how many times the system call that feature represents is called. As such, the range of values a feature held varied greatly. Some present system calls occurred only a few times where others were called hundreds of thousands of times.

The decision tree classifier is only feasible with numeric feature values by using thresholds or by making the values discrete. I choose to make them discrete by transforming all features into either a $0$ or a $1$. I calculated the average value a feature held across all the training examples. I considered to be "present" or $1$ if it's value is above the average and $0$ otherwise. I played with this threshold by introducing a hyper-parameter $\lambda$.

$$threshold_j = \lambda * average_j$$

This allowed me to vary how above-or-below the average a feature $j$ should be to be considered "present".

\section*{Cross-validation}
My random forest contained four different hyper-parameters, all of which were determined by using 6-fold cross validation.

\begin{table}[H]
{\renewcommand{\arraystretch}{1.2}%
\begin{tabular}{| c | c | c |}
\hline
Hyper-parameter & Description & Best Value\\
\hline
$\lambda$ & Decision boundary for discretization. & 0.34\\ \hline
N & Decision tree count. & 101\\ \hline
m & Size of training example sample. & 10,000\\ \hline
k & Number of randomly selected features. & 50\\ \hline
\end{tabular}}
\caption{The used hyper-parameters for the random forest classifier.}
\end{table}

If a system call was called at least 0.34 times the average, that feature was considered present an was given a value of $1$ and a value of $0$ otherwise. 

The random forest consisted of $101$ decision trees. Each decision trees was constructed with 50 randomly selected features and was trained on 10,0000 sampled with replacement training examples.

\section*{Performance}
sadfa

\section*{Improvements}

\section*{Other Classifiers Attempted}

\section*{What I Would Like To Try}

\section*{What I Learned}

\section*{References}
[1] Dimja\v{s}evi\'{c}, M., Atzeni, S., Ugrina, I. and Rakamaric, Z., 2016, March. Evaluation of Android Malware Detection Based on System Calls. In Proceedings of the 2016 ACM on International Workshop on Security And Privacy Analytics (pp. 1-8). ACM.

\end{document}