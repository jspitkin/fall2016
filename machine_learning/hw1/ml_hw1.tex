\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{float}
\usepackage{tikz}
\usetikzlibrary{arrows, quotes, trees}
\usepackage[margin=1in]{geometry}
\usepackage{fancyhdr}
\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt plus 1pt}
\setlength{\headheight}{13.6pt}
\newcommand\question[2]{\vspace{.25in}\hrule\textbf{#1: #2}\vspace{.5em}\hrule\vspace{.10in}}
\renewcommand\part[1]{\vspace{.10in}\textbf{(#1)}}
\pagestyle{fancyplain}
\lhead{\textbf{\NAME\ (\UID)}}
\chead{\textbf{HW\HWNUM}}
\rhead{CS 6350, \today}

\tikzstyle{block} = [rectangle, draw, fill=white!20,
    text width=3em, text centered, rounded corners, minimum height=2em]
\tikzstyle{line} = [draw, very thick, color=black!50, -latex']
\tikzstyle{leaf} = [circle, draw, fill=none, text width=1em, text centered, minimum height=1em]

\begin{document}\raggedright

\newcommand\NAME{Jake Pitkin}
\newcommand\UID{u0891770}
\newcommand\HWNUM{1}

\question{1}{Decision trees}

\emph{Note: Square nodes test for feature values and round leaf nodes specify the class labels.}

\part{1} Representing Boolean functions as decision trees.

\part{a} $(x_1 \lor x_2) \land x_3$

\hspace*{30 mm}\begin{tikzpicture}[
box/.style = {rectangle, draw, align=center},
level distance = 18mm,
level 1/.style = {sibling distance=66mm},
edge from parent/.style = {draw, -latex'},
edge from parent fork down
                        ]
\node [block] {$x_3$} 
	child { 
      node [leaf] {0} 
      edge from parent node[left] {0}  
	} 
    child { 
      node [block] {$x_1$} 
        child { 
          node [block] {$x_2$}
          child {
          	node [leaf] {0}
          	edge from parent node[left] {0}
          }
          child {
          	node [leaf] {1}
          	edge from parent node[right] {1}
          } 
          edge from parent node[left] {0} 
        }
        child {
        	node [leaf] {1}
        	edge from parent node[right] {1}
        }
        edge from parent node[right] {1}  
    }; 
\end{tikzpicture}

\part{b} $(x_1 \land x_2) \ xor\ (\neg x_1 \lor x_3)$

\hspace*{30 mm}\begin{tikzpicture}[
box/.style = {rectangle, draw, align=center},
level distance = 18mm,
level 1/.style = {sibling distance=66mm},
level 3/.style = {sibling distance = 40mm},
edge from parent/.style = {draw, -latex'},
edge from parent fork down
                        ]
\node [block] {$x_1$} 
	child { %level 2
      node [leaf] {1} 
      edge from parent node[left] {0}  
	} 
    child { %level 2
      node [block] {$x_2$} 
        child { %level 3
          node [block] {$x_3$}
          child { %level 4
          	node [leaf] {0}
          	edge from parent node[left] {0}
          }
          child { %level 4
          	node [leaf] {1}
          	edge from parent node[right] {1}
          } 
          edge from parent node[left] {0} 
        }
        child { %level 3
        	node [block] {$x_3$}
        	child { %level 4
        		node [leaf] {1}
        		edge from parent node[left] {0}
        	}
        	child { %level 4
        		node [leaf] {0}
        		edge from parent node [right] {1}
        	}
        	edge from parent node[right] {1}
        }
        edge from parent node[right] {1}  
    }; 
\end{tikzpicture}

\newpage
\part{c} The 2-of-3 function defined as follows: at least 2 of $\{x_1, x_2, x_3\}$ should be true for the \hspace*{13 mm} output to be true.

\hspace*{30 mm}\begin{tikzpicture}[
box/.style = {rectangle, draw, align=center},
level distance = 18mm,
level 1/.style = {sibling distance=66mm},
level 2/.style = {sibling distance=40mm},
level 3/.style = {sibling distance = 10mm},
edge from parent/.style = {draw, -latex'},
edge from parent fork down
                        ]
\node [block] {$x_1$} 
	child { %level 2
      node [block] {$x_2$}
      child { %level 3
      	node [leaf] {0}
      	edge from parent node[left] {0}
      }
      child { % level 3
      	node [block] {$x_3$}
      	child { %level 4
      		node[leaf] {0}
      		edge from parent node[left] {0}
      	}
      	child { %level 4
      		node[leaf] {1}
      		edge from parent node[right] {1}
      	}
      	edge from parent node[right] {1}
      }
      edge from parent node[left] {0}  
	} 
    child { %level 2
      node [block] {$x_2$} 
        child { %level 3
          node [block] {$x_3$}
          child { %level 4
          	node [leaf] {0}
          	edge from parent node[left] {0}
          }
          child { %level 4
          	node [leaf] {1}
          	edge from parent node[right] {1}
          } 
          edge from parent node[left] {0} 
        }
        child { %level 3
        	node [leaf] {1}
        	edge from parent node[right] {1}
        }
        edge from parent node[right] {1}  
    }; 
\end{tikzpicture}

\part{2} Pok\'{e}mon Go decision tree to determine whether a Pok\'{e}mon can be caught.

\part{a} There are 2 choices for Berry, 3 choices for Ball, 3 choices for Color, and 4 choices for type. This gives $2 * 3 * 3 * 4 = 72$ possible outputs. We are making a Boolean decision, so there are two ways to fill each of those 72 outputs giving $2^{72}$ possible functions.

\framebox[1.5\width]{$2^{72}$ possible functions} \par

\part{b} Entropy is defined as: $$H(S) = -p_+\log(p_+) - p_-\log(p_-)$$
The training set contains 16 examples, 8 of which result in a catch while the other 8 do not. 
$$H(Caught) = -\frac{8}{16}\log(\frac{8}{16})-\frac{8}{16}\log(\frac{8}{16}) = 1$$

\framebox[1.5\width]{$H(Caught) = 1$} \par
	
\part{c} Information gain is defined as:
$$Gain(S, A) = H(S)\  - \sum_{v\in Values(A)} \frac{|S_v|}{|S|} H(S_v)$$

Berry = Yes. 7 out of 16 examples. Caught = 6/7 - Not Caught = 1/7. $H(berry_{yes}) = 0.592$.
Berry = No. 9 out of 16 examples. Caught = 2/9 - Not Caught = 7/9. $H(berry_{no}) = 0.764$.
$$Gain(S, Berry) = 1 - ((\frac{7}{16})(0.592) + (\frac{9}{16})(0.764)) = 0.689$$

\framebox[1.5\width]{$Gain(S, Berry) = 0.311$} \par
\newpage

Ball = Pok\'{e}. 6 out of 16 examples. Caught = 1/6 - Not Caught = 5/6. $H(ball_{poke}) = 0.65$.

Ball = Great. 7 out of 16 examples. Caught = 4/7 - Not Caught = 3/7. $H(ball_{great}) = 0.985$.

Ball = Ultra. 3 out of 16 examples. Caught = 3/3 - Not Caught = 0/3. $H(ball_{ultra}) = 0$.
$$Gain(S, Ball) = 1 - ((\frac{6}{16})(0.65) + (\frac{7}{16})(0.985) + (\frac{3}{16})(0)) = 0.325$$

\framebox[1.5\width]{$Gain(S, Ball) = 0.325$} \par

Color = Green. 3 out of 16 examples. Caught = 2/3 - Not Caught = 1/3. $H(color_{green}) = 0.918$.

Color = Yellow. 7 out of 16 examples. Caught = 3/7 - Not Caught = 4/7. $H(color_{yellow}) = 0.985$.

Color = Red. 6 out of 16 examples. Caught = 3/6 - Not Caught = 3/6. $H(color_{red}) = 1$.
$$Gain(S, Color) = 1 - ((\frac{3}{16})(0.918) + (\frac{7}{16})(0.985) + (\frac{6}{16})(1)) = 0.022$$

\framebox[1.5\width]{$Gain(S, Color) = 0.022$} \par 

Type = Normal. 6 out of 16 examples. Caught = 3/6 - Not Caught = 3/6. $H(type_{normal}) = 1$.

Type = Water. 4 out of 16 examples. Caught = 2/4 - Not Caught = 2/4. $H(type_{water}) = 1$.

Type = flying. 4 out of 16 examples. Caught = 3/4 - Not Caught = 1/4. $H(type_{flying}) = 0.811$.

Type = psychic. 2 out of 16 examples. Caught = 0/2 - Not Caught = 2/2. $H(type_{psychic}) = 0$.

$$Gain(S, Type) = 1 - ((\frac{6}{16})(1) + (\frac{4}{16})(1) + (\frac{4}{16})(0.811) + (\frac{2}{16})(0)) = 0.172$$

\framebox[1.5\width]{$Gain(S, Type) = 0.172$} \par 

\part{d} When using the ID3 algorithm to create a decision tree, I would select the attribute Ball as the root of the tree.

\setlength{\fboxsep}{4pt}
\framebox[2.1\width]{Ball} \par 
\newpage
\part{e}
\hspace*{0 mm}\begin{tikzpicture}[
box/.style = {rectangle, draw, align=center},
level distance = 18mm,
level 1/.style = {sibling distance=66mm},
level 2/.style = {sibling distance=40mm},
level 3/.style = {sibling distance = 20mm},
edge from parent/.style = {draw, -latex'},
%edge from parent fork down
                        ]
\node [block] {$Ball$} 
	child { %level 2
      node [block] {Berry}
      child { %level 3
      	node [leaf] {no}
      	edge from parent node[left] {no}
      }
      child { % level 3
      	node [block] {Color}
      	child { %level 4
      		node[leaf] {yes}
      		edge from parent node[left] {green}
      	}
      	child { %level 4
      		node[leaf] {no}
      		edge from parent node {yellow}
      	}
      	child { %level 4
      		node[leaf] {no}
      		edge from parent node[right] {red}
      	}
      	edge from parent node[right] {yes}
      }
      edge from parent node[left] {Pok\'{e}}  
	} 
	child { %level 2
		node [block] {Berry}
		child { %level 3
			node [leaf] {no}
			edge from parent node[left] {no}
		}
		child { %level 3
			node [leaf] {yes}
			edge from parent node[right] {yes}
		}
		edge from parent node [left]{Great}
	}
    child { %level 2
      node [leaf] {yes} 
      edge from parent node[right] {Ultra}  
    }; 
\end{tikzpicture}
 \quad The label 'no' on Ball[Poke] $\rightarrow$ Berry[yes] $\rightarrow$ Color[yellow] was chosen arbitrarily. The training examples with Ball[Poke] $\rightarrow$ Berry[Yes] have even probability between a 'yes' and 'no' label. The color yellow has no training data so a random choice was made.

\part{f}

\begin{table}[H]
\centering
\begin{tabular}{| c c c c | c | c |}
\hline
Berry& Ball & Color & Type & Caught & Correct Prediction?\\
\hline
Yes & Great & Yellow & Psychic & Yes & Yes \\
Yes & Pok\'e & Green & Flying & No & No \\
No & Ultra & Red & Water & No & No \\
\hline
\end{tabular}
\caption{Predictions for test set}
\end{table}

Out of the three examples in the test set, my decision tree predicted only one correct.

\framebox[1.5\width]{Accuracy = $33\%$} \par 

\part{g} I think using a decision tree to classify if a Pok\'{e}mon will be caught or not is a good idea. The training set provided is very sparse and I would expect a better rate of accuracy given more training data.

\part{3} Using the Gini measure with the ID3 algorithm.

\qquad \part{a} Information gain is defined as:
$$Gain(S, A) = Gini(S)\  - \sum_{v\in Values(A)} \frac{|S_v|}{|S|} Gini(S_v)$$

Caught = Yes. 8 out of 16 examples. Caught = No. 8 out of 16 examples. $Gini(S) = 0.5$ \newline

Berry = Yes. 7 out of 16 examples. Caught = 6/7 - Not Caught = 1/7. $Gini(berry_{yes}) = 0.245$.
 
Berry = No. 9 out of 16 examples. Caught = 2/9 - Not Caught = 7/9. $Gini(berry_{no}) = 0.346$.
$$Gain(S, Berry) = 0.5 - ((\frac{7}{16})(0.245) + (\frac{9}{16})(0.346)) = 0.2$$

\framebox[1.5\width]{$Gain(S, Berry) = 0.2$} \par

Ball = Pok\'{e}. 6 out of 16 examples. Caught = 1/6 - Not Caught = 5/6. $Gini(ball_{poke}) = 0.278$.

Ball = Great. 7 out of 16 examples. Caught = 4/7 - Not Caught = 3/7. $Gini(ball_{great}) = 0.49$.

Ball = Ultra. 3 out of 16 examples. Caught = 3/3 - Not Caught = 0/3. $Gini(ball_{ultra}) = 0$.
$$Gain(S, Ball) = 0.5 - ((\frac{6}{16})(0.278) + (\frac{7}{16})(0.49) + (\frac{3}{16})(0)) = 0.181$$

\framebox[1.5\width]{$Gain(S, Ball) = 0.181$} \par

Color = Green. 3 out of 16 examples. Caught = 2/3 - Not Caught = 1/3. $Gini(color_{green}) = 0.444$.

Color = Yellow. 7 out of 16 examples. Caught = 3/7 - Not Caught = 4/7. $Gini(color_{yellow}) = 0.49$.

Color = Red. 6 out of 16 examples. Caught = 3/6 - Not Caught = 3/6. $Gini(color_{red}) = 0.5$.
$$Gain(S, Color) = 0.5 - ((\frac{3}{16})(0.444) + (\frac{7}{16})(0.49) + (\frac{6}{16})(0.5)) = 0.015$$

\framebox[1.5\width]{$Gain(S, Color) = 0.015$} \par 

Type = Normal. 6 out of 16 examples. Caught = 3/6 - Not Caught = 3/6. $Gini(type_{normal}) = 0.5$.

Type = Water. 4 out of 16 examples. Caught = 2/4 - Not Caught = 2/4. $Gini(type_{water}) = 0.5$.

Type = flying. 4 out of 16 examples. Caught = 3/4 - Not Caught = 1/4. $Gini(type_{flying}) = 0.375$.

Type = psychic. 2 out of 16 examples. Caught = 0/2 - Not Caught = 2/2. $Gini(type_{psychic}) = 0$.

$$Gain(S, Type) = 0.5 - ((\frac{6}{16})(0.5) + (\frac{4}{16})(0.5) + (\frac{4}{16})(0.375) + (\frac{2}{16})(0)) = 0.094$$

\framebox[1.5\width]{$Gain(S, Type) = 0.094$} \par 



\qquad \part{b} I would pick Berry as the root of my decision tree. When using the Gini measure to calculate the information gain of each attribute, Berry has the highest information gain. This is different than when entropy is used to calculate information gain. When entropy is used, Ball is selected as the root.

\framebox[1.5\width]{Berry - different than when entropy is used}
\newpage
\question{2}{Linear Classifiers}

\part{1} Weights $\mathbf{w}$ and a bias $\mathbf{b}$ classifies the dataset from problem 1.
$$
\mathbf{w} = \begin{bmatrix}
0 \\
0 \\
1 \\
1 \\ 	
\end{bmatrix}
 and \ b = -1
$$
Looking at the dataset, one possible way to classify the data is either $x_3$ or $x_4$ must be set to produce an output of $1$. Otherwise, the output is $-1$.

\part{2} The table below details the accuracy of the linear classifier from part 1 on the dataset from part 2:
 \begin{center}
    \begin{tabular}{cccc|c|c}
      $x_1$ & $x_2$ & $x_3$ & $x_4$ & $o$ & $prediction$\\ \hline
      1 & 0 & 1 & 1 & 1 & 1\\
      0 & 1 & 0 & 1 & 1 & 1\\
      1 & 0 & 1 & 0 & 1 & 1\\
      1 & 1 & 0 & 0 & 1 & -1\\
      1 & 1 & 1 & 1 & 1 & 1\\
      1 & 1 & 1 & 0 & 1 & 1\\
      0 & 0 & 1 & 0 & -1 & 1\\
    \end{tabular}
  \end{center}
\framebox[1.5\width]{Accuracy = 5/7 or 71.4\%}

\part{3} Weights $\mathbf{w}$ and a bias $\mathbf{b}$ classifies the combined dataset from part 1, 2, and 3.
$$
\mathbf{w} = \begin{bmatrix}
1 \\
0 \\
0 \\
1 \\ 	
\end{bmatrix}
 and \ b = -1
$$
Looking at the combined dataset, we can see that either $x_1$ or $x_4$ must be set to produce an output of $1$. Otherwise, the output is $-1$.

\question{3}{Setting A}

\part{1a} I implemented my decision tree and the ID3 algorithm in Python. Python felt like the right language choice as it handles manipulating data well. For the representation of the tree, I used a Node and Edge class. I felt like this was the best choice since a node can have more than two children (binary tree won't do) and edges have labels on them. I wrote the file reading and handling with k-folding in mind, allowing multiple files as input data. This meant more development time upfront, but by making the code a little more generic I saved time come experimentation time.
\newpage
For questions \textbf{1b}, \textbf{1c}, and \textbf{1d} my decision tree was trained on the \textbf{SettingA/training.data} file.

\part{1b} For the \textbf{SettingA/training.data} file, I had a $0\%$ error rate with 3822/3822 examples correctly classified.

\framebox[1.5\width]{Error rate = $0\%$}

\part{1c} For the \textbf{SettingA/test.data} file, I had a $0\%$ error rate with 3822/3822 examples correctly classified.

\framebox[1.5\width]{Error rate = $0\%$}

\part{1d} The maximum depth of my decision tree is three decisions.

\framebox[1.5\width]{Maximum depth = $3$}

\part{2a}
\begin{table}[H]
\centering
\begin{tabular}{| c | c | c |}
\hline
Tree Size& Cross-validation Accuracy & Standard Deviation\\
\hline
1 & 97.65\% & 0.051\\
2 & 98.14\% & 0.042\\
3 & 98.14\% & 0.042\\
4 & 98.14\% & 0.042\\
5 & 98.14\% & 0.042\\
10 & 98.14\% & 0.042\\
15 & 98.14\% & 0.042\\
20 & 98.14\% & 0.042\\
\hline
\end{tabular}
\caption{Results for 6-fold cross-validation with various maximum tree sizes}
\end{table}
Above are the results for 6-fold cross validation using the \textbf{SettingA/training\_0X.data} files. We can see from the chart that the best accuracy comes from using a tree of size 2 or larger. I would choose a tree of size two, because a more sparse decision tree can help avoid overfitting.

\part{2b} I train a decision tree, with a maximum of size 2, on \textbf{SettingA/training.data}. When testing the \textbf{SettingA/test.data} file, I get an error rate of $0.22\%$ with 1818/1822 examples correctly classified.

\framebox[1.5\width]{Error rate = $0.22\%$}

\question{3}{Setting B}

For questions \textbf{1a}, \textbf{1b}, \textbf{1c}, \textbf{1d} and \textbf{1e} my decision tree was trained on the \textbf{SettingB/training.data} file.

\part{1a} For the \textbf{SettingB/training.data} file, I had a $0\%$ error rate with 3822/3822 examples correctly classified.

\framebox[1.5\width]{Error rate = $0\%$}

\part{1b} For the \textbf{SettingB/test.data} file I had a $6.3\%$ error rate with 1707/1822 examples correctly classified.

\framebox[1.5\width]{Error rate = $6.3\%$}

\part{1c} For the \textbf{SettingA/training.data} file I had a $0.05\%$ error rate with 3820/3822 examples correctly classified.

\framebox[1.5\width]{Error rate = $0.05\%$} 

\part{1d} For the \textbf{SettingA/test.data} file I had a $0.1\%$ error rate with 1820/1822 examples correctly classified.

\framebox[1.5\width]{Error rate = $0.1\%$}

\part{1e} The maximum depth of my decision tree is nine decisions.

\framebox[1.5\width]{Maximum depth = $9$}

\part{2a}
\begin{table}[H]
\centering
\begin{tabular}{| c | c | c |}
\hline
Tree Size& Cross-validation Accuracy & Standard Deviation\\
\hline
1 & 92.86\% & 0.0434\\
2 & 90.21\% & 0.0953\\
3 & 92.26\% & 0.043\\
4 & 92.7\% & 0.02\\
5 & 92.52\% & 0.0239\\
10 & 92.28\% & 0.0244\\
15 & 92.28\% & 0.0244\\
20 & 92.28\% & 0.0244\\
\hline
\end{tabular}
\caption{Results for 6-fold cross-validation with various maximum tree sizes}
\end{table}
Above are the results for 6-fold cross validation using the \textbf{SettingA/training\_0X.data} files. We can see from the chart that the best accuracy comes from using a tree of size 1. I would choose a tree of size one, because a more sparse decision tree can help avoid overfitting and it has the best accuracy. Additionally, the standard deviations between the cross-validations errors is lower than a tree of size two, which is another good candidate.

\framebox[1.5\width]{Optimal tree size = 1} 

\part{2b} I train a decision tree, with a maximum of size 1, on \textbf{SettingB/training.data}. When testing the \textbf{SettingB/test.data} file, I get an error rate of $6.42\%$ with 1705/1822 examples correctly classified.

\framebox[1.5\width]{Error rate = $6.42\%$}
\newpage

\question{3}{Setting C}

\part{1} I added an additional parameter to the constructor for my decision tree that allows for the user to decide between the three methods of replacement if an unknown value is encountered. \newline\newline
	\textbf{Method 1:} I use the most common value of that missing attribute in the data. This is done after the data is read in, I then scan over the data looking for missing features and replace them. \newline
	
	\textbf{Method 2:} Similar to method two, after reading in the data I scan it for missing features. This time replacing it with the most common value of that attribute given it's label.
	
	\textbf{Method 3:} For this method, I simply added "?" as a possible feature value for all the features in the dataset.

\part{2}
\begin{table}[H]
\centering
\begin{tabular}{| c | c | c |}
\hline
Method & Cross-validation Accuracy & Standard Deviation\\
\hline
1 & 99.17\% & 0.0185\\
2 & 99.17\% & 0.0185\\
3 & 99.17\% & 0.0185\\
\hline
\end{tabular}
\caption{Results for 6-fold cross-validation with various replacement methods}
\end{table}
I saw no variance between the three methods of replacement. My theory is, based on my findings from my earlier experiments, the decision trees created with the ID3 algorithm are shallow. Meaning they rely on only one or two features to make a classification. Because of this, missing values had very little weight on the classification of examples. It would take many missing feature values of these dominate one or two features to create a statistical difference in the classifications.

\part{2b} I train a decision tree, with a maximum of size 1, on \textbf{SettingC/training.data}. When testing the \textbf{SettingC/test.data} file, I get an error rate of $1.34\%$ with 2654/2690 examples correctly classified.

\framebox[1.5\width]{Error rate = $1.34\%$}
\end{document}

