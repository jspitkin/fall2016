%    2. Write your answers in section "B" below. Precede answers for all 
%       parts of a question with the command "\question{n}{desc}" where n is
%       the question number and "desc" is a short, one-line description of 
%       the problem. There is no need to restate the problem.
%    3. If a question has multiple parts, precede the answer to part x with the
%       command "\part{x}".
%    4. If a problem asks you to design an algorithm, use the commands
%       \algorithm, \correctness, \runtime to precede your discussion of the 
%       description of the algorithm, its correctness, and its running time, respectively.
%    5. You can include graphics by using the command \includegraphics{FILENAME}
%
\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage{fancyhdr}
\usepackage{float}
\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt plus 1pt}
\setlength{\headheight}{13.6pt}
\newcommand\question[2]{\vspace{.25in}\hrule\textbf{#1: #2}\vspace{.5em}\hrule\vspace{.10in}}
\renewcommand\part[1]{\vspace{.10in}\textbf{(#1)}}
\pagestyle{fancyplain}
\lhead{\textbf{\NAME\ (\UID)}}
\chead{\textbf{HW\HWNUM}}
\rhead{CS 5340, \today}
\begin{document}\raggedright

\newcommand\NAME{Jake Pitkin}
\newcommand\UID{u0891770}
\newcommand\HWNUM{2}

\question{Problem 1}{Laplace Smoothing}

Let $|V| = 5$ where $V$ is the vocabulary and  $N = \sum\limits_{i}^{|V|} freq(w_i) = 1,200$ where $N$ is the number of words in the corpus. Without smoothing, the probabilities of each noun in the corpus are:

\qquad$P(maple) = \frac{freq(maple)}{N} = \frac{600}{1,200} = 0.5$

\qquad$P(oak) = \frac{freq(oak)}{N} = \frac{400}{1,200} = 0.333$

\qquad$P(pine) = \frac{freq(pine)}{N} = \frac{180}{1,200} = 0.15$

\qquad$P(spruce) = \frac{freq(spruce)}{N} = \frac{20}{1,200} = 0.167$

\qquad$P(aspen) = \frac{freq(aspen)}{N} = \frac{0}{1,200} = 0$

Using Laplace smoothing (aka: Add-One Smoothing), the new frequencies for each noun:

\qquad$new\_freq(maple) = (freq(maple) + 1) * \frac{N}{N + V} = (600 + 1) * \frac{1,200}{1,200 + 5} = \frac{721,200}{1,205} = 598.506$

\qquad$new\_freq(oak) = (freq(oak) + 1) * \frac{N}{N + V} = (400 + 1) * \frac{1,200}{1,200 + 5} = \frac{481,200}{1,205} = 399.336$

\qquad$new\_freq(pine) = (freq(pine) + 1) * \frac{N}{N + V} = (180 + 1) * \frac{1,200}{1,200 + 5} = \frac{217,200}{1,205} = 180.249$

\qquad$new\_freq(spruce) = (freq(spruce) + 1) * \frac{N}{N + V} = (20 + 1) * \frac{1,200}{1,200 + 5} = \frac{25,200}{1,205} = 20.913$

\qquad$new\_freq(aspen) = (freq(aspen) + 1) * \frac{N}{N + V} = (0 + 1) * \frac{1,200}{1,200 + 5} = \frac{1,200}{1,205} = 0.966$

Given these new frequencies, the new probabilities are calculated:

\qquad$new\_P(maple) = \frac{new\_freq(maple)}{N} = \frac{598.506}{1,200} = 0.499$

\qquad$new\_P(oak) = \frac{new\_freq(oak)}{N} = \frac{399.336}{1,200} = 0.333$

\qquad$new\_P(pine) = \frac{new\_freq(pine)}{N} = \frac{180.249}{1,200} = 0.15$

\qquad$new\_P(spruce) = \frac{new\_freq(spruce)}{N} = \frac{20.913}{1,200} = 0.017$

\qquad$new\_P(aspen) = \frac{new\_freq(aspen)}{N} = \frac{0.996}{1,200} = 0.001$

Completing the table:

\begin{table}[H]
\centering
\begin{tabular}{| c | c | c | | c | c |}
\hline
\textbf{Noun} & \textbf{Freq.} & \textbf{Unsmoothed Prob.} & \textbf{Smoothed Freq.} & \textbf{Smoothed Prob.}\\
\hline
\hline
maple & 600 & 0.5 & 598.506 & 0.499 \\
\hline
oak & 400 & 0.333 & 399.336 & 0.333 \\
\hline
pine & 180 & 0.15 & 180.249 & 0.15 \\
\hline
spruce & 20 & 0.017 & 20.913 & 0.017 \\
\hline
aspen & 0 & 0 & 0.996 & 0.001 \\ 
\hline
\end{tabular}
\caption{Unsmoothed and smoothed probabilities and frequencies.}
\end{table}
\newpage

\question{2}{Grammars and Recursive Transition Networks}

\part{a} Grammar A and Grammar B - \textbf{DIFFERENT}

Grammar A requires the NP to begin with an article but Grammar B does not. 

An example POS tag sequence \textbf{accepted by Grammar B} and not by Grammar A is: \textbf{noun}.\newline

\part{b} Grammar A and Grammar C - \textbf{DIFFERENT}

Grammar C requires one or more adjectives after the article but Grammar A requires zero or more adjectives after the article. 

An example POS tag sequence \textbf{accepted by Grammar A} and not by Grammar C is: \textbf{art noun}. \newline

\part{c} Grammar A and RTN-2 - \textbf{DIFFERENT}

Grammar A requires the NP to begin with an article but RTN-2 does not.

An example POS tag sequence \textbf{accepted by RTN-2} and not by Grammar A is: \textbf{noun}.\newline

\part{d} Grammar A and RTN-3 - \textbf{DIFFERENT}

RTN-3 requires one or more adjectives after the article but Grammar A requires zero or more adjectives after the article.

An example POS tag sequence \textbf{accepted by Grammar A} and not by RTN-3 is: \textbf{art noun}.\newline

\part{e} Grammar B and RTN-2 - \textbf{SAME}\newline

\part{f} Grammar C and RTN-1 - \textbf{DIFFERENT}

Grammar C requires the NP to end with one or more nouns but RTN-1 does not.

An example POS tag sequence \textbf{accepted by RTN-1} and not by Grammar C is: \textbf{art adj}.\newline

\part{g} Grammar C and RTN-3 - \textbf{SAME}\newline

\part{h} RTN-1 and RTN-3 - \textbf{DIFFERENT}

RTN-3 requires the NP to end with one or more nouns but RTN-1 does not.

An example POS tag sequence \textbf{accepted by RTN-1} and not by Grammar C is: \textbf{art adj}.

\newpage

\question{3}{N-Gram Probabilities}

Let $k$ denote the number of distinct lexical unigrams in the text corpus. \newline
\part{a} $P(the) = \frac{freq(the)}{\sum\limits_{i}^{k} freq(w_i)} = \ $ \framebox[2\width]{$\cfrac{5}{34}$}

Let $k$ denote the number of distinct POS unigrams in the text corpus. \newline
\part{b} $P(the) = \frac{freq(VERB)}{\sum\limits_{i}^{k} freq(T_i)} = \ $\framebox[2\width]{$\cfrac{6}{34}$}

Let $freq(W_{n-m},...,W_{n-1}, W_{n})$ denote the frequency of the phrase $W_{n-m},...,W_{n-1}, W_{n}$ appearing in the text corpus. \newline
\part{c} $P(young \ | \ girl) = \frac{freq(W_{n-1}, W_n)}{freq(W_{n-1})} = \frac{freq(girl \ young)}{freq(girl)} = \ $\framebox[2\width]{$\cfrac{0}{3}$}

\part{d} $P(girl \ | \ young) = \frac{freq(W_{n-1}, W_n)}{freq(W_{n-1})} = \frac{freq(young \ girl)}{freq(young)} = \ $\framebox[2\width]{$\cfrac{2}{3}$}

\part{e} $P(and \ | \ women) = \frac{freq(W_{n-1}, W_n)}{freq(W_{n-1})} = \frac{freq(women \ and)}{freq(women)} = \ $\framebox[2\width]{$\cfrac{1}{3}$}

\part{f} $P(thanked \ | \ young \ girl) = \frac{freq(W_{n-2}, W_{n-1}, W_n)}{freq(W_{n-2}, W_{n-1})} = \frac{freq(young \ girl \ thanked)}{freq(young \ girl)} = \ $\framebox[2\width]{$\cfrac{0}{2}$}

\part{g} $P(five \ | \ gave \ her) = \frac{freq(W_{n-2}, W_{n-1}, W_n)}{freq(W_{n-2}, W_{n-1})} = \frac{freq(gave \ her \ five)}{freq(gave \ her)} = \ $\framebox[2\width]{$\cfrac{1}{2}$}

Let $C(event)$ denote the count of the given event occurring in the text corpus. \newline
\part{h} $P(the \ | \ ART) = \frac{C(w_i \ with \ tag \ T_i)}{C(any \ word \ with \ tag \ T_i)} = \frac{C(the \ with \ tag \ ART)}{C(any \ word \ with \ tag \ ART)} = \ $\framebox[2\width]{$\cfrac{5}{8}$}

\part{i} $P(cross \ | \ NOUN) = \frac{C(w_i \ with \ tag \ T_i)}{C(any \ word \ with \ tag \ T_i)} = \frac{C(cross \ with \ tag \ NOUN)}{C(any \ word \ with \ tag \ NOUN)} = \ $\framebox[2\width]{$\cfrac{0}{9}$}

\part{j} $P(thanked \ | \ NOUN) = \frac{C(thanked \ with \ tag \ NOUN)}{C(any \ word \ with \ tag \ T_i)} = \frac{C(thanked \ with \ tag \ NOUN)}{C(any \ word \ with \ tag \ NOUN)} = \ $\framebox[2\width]{$\cfrac{0}{9}$}

\part{k} $P(NUM \ | \ PRO) = \frac{C(T_i \ immediately \ follows \ T_{i-1})}{C(any \ tag \ immediately \ follows \ T_{i- 1})} = \frac{C(NUM \ immediately \ follows \ PRO)}{C(any \ tag \ immediately \ follows \ PRO)} = \ $\framebox[2\width]{$\cfrac{1}{2}$}

\part{l} $P(ART \ | \ VERB) = \frac{C(T_i \ immediately \ follows \ T_{i-1})}{C(any \ tag \ immediately \ follows \ T_{i- 1})} = \frac{C(ART \ immediately \ follows \ VERB)}{C(any \ tag \ immediately \ follows \ VERB)} = \ $\framebox[2\width]{$\cfrac{4}{6}$}

\newpage

\question{4}{Viterbi Algorithm}

\part{a} $P(light=VERB) = P(VERB\ | \ \phi) * P(light \ | \ VERB) = 0.25 * 0.5 =$ \framebox[2\width]{$0.125$}

\part{b} $P(light=NOUN) = P(NOUN \ | \ \phi) * P(light \ | \ NOUN) = 0.7 * 0.6 =$ \framebox[2\width]{$0.42$}

\part{c} $P(light=ADJ) = P(ADJ \ | \ \phi) * P(light \ | \ ADJ) = 0.15 * 0.2 =$ \framebox[2\width]{$0.03$}

\part{d} $P(shows=VERB) = P(shows \ | \ V) * MAX[(P(light=V) * P(V \ | \ V)), (P(light=N) * P(V \ | \ N)), (P(light=A) * P(V \ | \ A))]$

$P(shows=VERB) = 0.3 * MAX[(0.125 * 0.4), (0.42 * 0.5), (0.03, 0.1)] = $ \framebox[2\width]{$0.063$} 

\part{e} $P(shows=NOUN) = P(shows \ | \ N) * MAX[(P(light=V) * P(V \ | \ V)), (P(light=N) * P(V \ | \ N)), (P(light=A) * P(VE \ | \ A))]$

$P(shows=NOUN) = 0.4 * MAX[(0.125 * 0.3), (0.42 * 0.8), (0.03, 0.6)] =$ \framebox[2\width]{$0.1344$}

\part{f} $P(shows=ADJ) = P(shows \ | \ A) * MAX[(P(light=V) * P(V \ | \ V)), (P(light=N) * P(V \ | \ N)), (P(light=A) * P(V \ | \ A))]$

$P(shows=ADJ) = 0.1 * MAX[(0.125 * 0.7), (0.42 * 0.2), (0.03, 0.9)] =$ \framebox[2\width]{$0.00875$}

The most likely sequence of POS tags for the sentence \textit{"Light shows"} is Light=NOUN and shows=NOUN.

\framebox[2\width]{light=NOUN and shows=NOUN}

\end{document}
